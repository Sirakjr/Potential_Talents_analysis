{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b0f622-7d13-46e9-bc55-8313bdcf5199",
   "metadata": {},
   "source": [
    "# Potential Talents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604cbed-1cd1-4412-9bb8-484304571c4b",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "As a talent sourcing and management company, we are interested in finding talented individuals for sourcing these candidates to technology companies. Finding talented candidates is not easy, for several reasons. The first reason is one needs to understand what the role is very well to fill in that spot, this requires understanding the client’s needs and what they are looking for in a potential candidate. The second reason is one needs to understand what makes a candidate shine for the role we are in search for. Third, where to find talented individuals is another challenge.\n",
    "\n",
    "The nature of our job requires a lot of human labor and is full of manual operations. Towards automating this process we want to build a better approach that could save us time and finally help us spot potential candidates that could fit the roles we are in search for. Moreover, going beyond that for a specific role we want to fill in we are interested in developing a machine learning powered pipeline that could spot talented individuals, and rank them based on their fitness.\n",
    "\n",
    "We are right now semi-automatically sourcing a few candidates, therefore the sourcing part is not a concern at this time but we expect to first determine best matching candidates based on how fit these candidates are for a given role. We generally make these searches based on some keywords such as “full-stack software engineer”, “engineering manager” or “aspiring human resources” based on the role we are trying to fill in. These keywords might change, and you can expect that specific keywords will be provided to you.\n",
    "\n",
    "Assuming that we were able to list and rank fitting candidates, we then employ a review procedure, as each candidate needs to be reviewed and then determined how good a fit they are through manual inspection. This procedure is done manually and at the end of this manual review, we might choose not the first fitting candidate in the list but maybe the 7th candidate in the list. If that happens, we are interested in being able to re-rank the previous list based on this information. This supervisory signal is going to be supplied by starring the 7th candidate in the list. Starring one candidate actually sets this candidate as an ideal candidate for the given role. Then, we expect the list to be re-ranked each time a candidate is starred.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544b9e8-81d2-4846-b02b-bf346050952a",
   "metadata": {},
   "source": [
    "## Data Description:\n",
    "\n",
    "The data comes from our sourcing efforts. We removed any field that could directly reveal personal details and gave a unique identifier for each candidate.\n",
    "\n",
    "Attributes:\n",
    "id : unique identifier for candidate (numeric)\n",
    "\n",
    "job_title : job title for candidate (text)\n",
    "\n",
    "location : geographical location for candidate (text)\n",
    "\n",
    "connections: number of connections candidate has, 500+ means over 500 (text)\n",
    "\n",
    "Output (desired target):\n",
    "fit - how fit the candidate is for the role? (numeric, probability between 0-1)\n",
    "\n",
    "Keywords: “Aspiring human resources” or “seeking human resources”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a6cbf-fba2-4907-8979-1840db0acefc",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "Predict how fit the candidate is based on their available information (variable fit)\n",
    "\n",
    "## Success Metric(s):\n",
    "\n",
    "Rank candidates based on a fitness score.\n",
    "\n",
    "Re-rank candidates when a candidate is starred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5201834-6cd8-4537-a9ca-f1e8c1fb3fa8",
   "metadata": {},
   "source": [
    "## Imports and Preprocessing\n",
    "Let's start by importing necessary libraries and packages for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6b0dbb-a31e-4777-afed-1315a17964eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sirak\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3511b85a-d33c-4a60-9bfd-08dda4772d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8079e54-2754-4d3c-8b2f-a061de749e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 104 candidates\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's load the data\n",
    "data = pd.read_csv('data/potential_talents_data.csv')\n",
    "print(f\"Loaded {len(data)} candidates\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c0e36-5466-499e-9c6a-67ec64b53c5c",
   "metadata": {},
   "source": [
    "Now let's start the preprocessing by creating a custom function which will convert the text to lowercase, remove punctuation and extra whitespaces, remove the stopwords, tokenize and finally lemmatize the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a3970c-a961-412b-b5bb-5b8e61ba4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    #convert to lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text=re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text=re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #Tokenize\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "    # Create lemmatizer object\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #remove stopwords and lemmatize\n",
    "    tokens= [lemmatizer.lemmatize(token) for token in tokens\n",
    "    if token not in set(stopwords.words('english')) ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9aa5124-846b-4b1e-a205-a03ea0e4900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add a new column 'job_title_preprocessed' to our dataset by applying the preprocess_text function\n",
    "data['job_title_preprocessed']=data['job_title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf68d15-a80f-4768-b3a4-2bbc3ab6f92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_title_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>2019 ct bauer college business graduate magna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>native english teacher epik english program korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>aspiring human resource professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>people development coordinator ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>advisory board member celal bayar university</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1  Native English Teacher at EPIK (English Progra...   \n",
       "2              Aspiring Human Resources Professional   \n",
       "3             People Development Coordinator at Ryan   \n",
       "4    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              job_title_preprocessed  \n",
       "0  2019 ct bauer college business graduate magna ...  \n",
       "1  native english teacher epik english program korea  \n",
       "2               aspiring human resource professional  \n",
       "3                people development coordinator ryan  \n",
       "4       advisory board member celal bayar university  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['job_title', 'job_title_preprocessed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae3d67-4b14-45ac-8927-53787d06d0c4",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66ed0c-0162-408c-8032-a846d7552328",
   "metadata": {},
   "source": [
    "For word embeddings we'll try a few methods and see which one works the best: Bag of Words, TF-IDF, Bert and Sbert. We'll create custom functions for each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2517c6a2-a291-4c49-b343-a39717bf77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the Bag of Words method\n",
    "\n",
    "def bag_of_words_similarity(data, target_string):\n",
    "    #create BoW vectorizer\n",
    "    vectorizer=CountVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "\n",
    "    # combine job titles with target string\n",
    "    all_texts=list(data['job_title_preprocessed'])+[target_string]\n",
    "    bow_matrix=vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    #Calculate similarity between each job title and target\n",
    "    job_title_matrix=bow_matrix[:-1] # All except last (target)\n",
    "    target_vector = bow_matrix[-1:] # Last row (target)\n",
    "\n",
    "    similarities=cosine_similarity(job_title_matrix, target_vector).flatten()\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648dd47b-6b25-4911-8086-c25b5760d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next on the list is TF-IDF\n",
    "\n",
    "def tfidf_similarity(data, target_string):\n",
    "    #create TF-IDF vectorizer\n",
    "    vectorizer=TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
    "\n",
    "    #Combine job titles with target string\n",
    "    all_texts=list(data['job_title_preprocessed'])+[target_string]\n",
    "    tfidf_matrix=vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    # Calculate similarity between each job title and target\n",
    "    job_titles_matrix = tfidf_matrix[:-1]  # All except last (target)\n",
    "    target_vector = tfidf_matrix[-1:]  # Last row (target)\n",
    "        \n",
    "    similarities = cosine_similarity(job_titles_matrix, target_vector).flatten()\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d0e8da-b5b1-451e-849f-bff4568e8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT similarity\n",
    "\n",
    "def bert_similarity(data, target_string):\n",
    "    #Load BERT model and tokenizer\n",
    "    tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model=BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "\n",
    "    #Calculate BERT embeddings for job titles\n",
    "    doc_embeddings=[]\n",
    "    for text in data['job_title']:\n",
    "        if pd.isna(text):\n",
    "            doc_embeddings.append(np.zeros(768))\n",
    "            continue\n",
    "\n",
    "        inputs=tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs=model(**inputs)\n",
    "            embedding=outputs.last_hidden_state[:,0,:].numpy().flatten()\n",
    "            doc_embeddings.append(embedding)\n",
    "\n",
    "    #Calculate BERT embedding for target\n",
    "    target_inputs=tokenizer(target_string, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_outputs=model(**target_inputs)\n",
    "        target_embedding=target_outputs.last_hidden_state[:,0,:].numpy().flatten()\n",
    "\n",
    "    #Calculate similarities\n",
    "    similarities=[]\n",
    "    for doc_embedding in doc_embeddings:\n",
    "        similarity=np.dot(doc_embedding, target_embedding) / (np.linalg.norm(doc_embedding) * np.linalg.norm(target_embedding) + 1e-8)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    return np.array(similarities)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3078af39-7b77-4abb-ab79-ca9799a25a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And last, but not least SBERT\n",
    "\n",
    "def sbert_similarity(data, target_string):\n",
    "\n",
    "    #Load SBERT model\n",
    "    sbert_model=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #Get embeddings for job titles\n",
    "    job_titles=data['job_title'].fillna('').tolist()\n",
    "    job_embeddings=sbert_model.encode(job_titles)\n",
    "\n",
    "    #get embedding for target\n",
    "    target_embedding = sbert_model.encode([target_string])\n",
    "\n",
    "    #Calculate similarities\n",
    "    similarities=cosine_similarity(job_embeddings, target_embedding).flatten()\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c587a3b-bd22-4089-a6d4-291e9cd42906",
   "metadata": {},
   "source": [
    "## Ranking and Reranking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd4c2a-a86f-4a54-9435-1c34c3d91f26",
   "metadata": {},
   "source": [
    "Before moving to comparing our embedding methods, let's build functions for ranking and reranking our candidates based on a embedding method that was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6529bd7b-b144-4553-bece-ac6d179328de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking candidates first\n",
    "\n",
    "def rank_candidates(data, target_string, method='tfidf', starred_candidates=None):\n",
    "    \"\"\"\n",
    "    method (str): Embedding method ('bow', 'tfidf', 'bert', 'sbert')\n",
    "    starred_candidates (list): List of candidate IDs that have been starred\n",
    "    \"\"\"\n",
    "    #Preprocessing the target string\n",
    "    target_processed=preprocess_text(target_string)\n",
    "\n",
    "    # Calculate similarities based on method\n",
    "    if method == 'bow':\n",
    "        similarities=bag_of_words_similarity(data, target_processed)\n",
    "    elif method == 'tfidf':\n",
    "        similarities = tfidf_similarity(data, target_processed)\n",
    "    elif method == 'bert':\n",
    "        similarities = bert_similarity(data, target_string)  \n",
    "    elif method == 'sbert':\n",
    "        similarities = sbert_similarity(data, target_string)  \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Available methods: 'bow', 'tfidf', 'bert', 'sbert'\")\n",
    "\n",
    "    # Normalize similarities to 0-1 range\n",
    "    scaler=MinMaxScaler()\n",
    "    similarities_norm=scaler.fit_transform(similarities.reshape(-1,1)).flatten()\n",
    "\n",
    "    # Create ranking dataframe\n",
    "    ranking_df=data.copy()\n",
    "    ranking_df['similarity_score'] = similarities_norm\n",
    "    ranking_df['rank'] = ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "\n",
    "    # Apply re-ranking if starred candidates provided\n",
    "    if starred_candidates:\n",
    "        ranking_df=rerank_with_starred(ranking_df, starred_candidates)\n",
    "\n",
    "    # Sort by rank\n",
    "    ranking_df=ranking_df.sort_values('rank').reset_index(drop=True)\n",
    "\n",
    "    return ranking_df\n",
    "\n",
    "# Now let's implement the rerank with starred function\n",
    "\n",
    "def rerank_with_starred(ranking_df, starred_candidates):\n",
    "\n",
    "    # Get features of starred candidates\n",
    "    starred_mask=ranking_df['id'].isin(starred_candidates)\n",
    "    starred_features=ranking_df[starred_mask]['similarity_score']\n",
    "\n",
    "    if len(starred_candidates) == 0:\n",
    "        return ranking_df\n",
    "\n",
    "    # Calculate average similarity of starred candidates\n",
    "    starred_avg = starred_features.mean()\n",
    "\n",
    "    # Boost scores for candidates similar to starred ones\n",
    "    for idx, row in ranking_df.iterrows():\n",
    "        candidate_score=row['similarity_score']\n",
    "\n",
    "        # Calculate similarity to starred candidates' average\n",
    "        similarity_to_starred=1-abs(candidate_score - starred_avg)\n",
    "\n",
    "        # Apply boost\n",
    "        boost_factor= 1 + 0.3 * similarity_to_starred\n",
    "        ranking_df.loc[idx, 'similarity_score'] = candidate_score * boost_factor\n",
    "\n",
    "    # Re-rank\n",
    "    ranking_df['rank']=ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "\n",
    "    return ranking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972cbbf-f22e-47b7-9f33-4bbb67ef7e61",
   "metadata": {},
   "source": [
    "## Comparing methods and choosing the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41156f83-53f3-4b8f-8db9-fb6632872e5c",
   "metadata": {},
   "source": [
    "Now we're ready to create functions for comparing our methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28b191a-843c-45f0-acfd-2c86630d872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, comparing the methods\n",
    "def compare_methods(data, target_string, starred_candidates=None):\n",
    "    methods = ['bow', 'tfidf', 'bert', 'sbert']\n",
    "    results = {}\n",
    "\n",
    "    # Comparing methods for target\n",
    "    for method in methods:\n",
    "        try:\n",
    "            ranking=rank_candidates(data, target_string, method, starred_candidates)\n",
    "\n",
    "            # Store top 10 results\n",
    "            top10= ranking.head(10)[['rank', 'id', 'job_title', 'similarity_score']]\n",
    "            results[method]={\n",
    "                'ranking': ranking,\n",
    "                'top_10': top10,\n",
    "                'avg_score': ranking['similarity_score'].mean(),\n",
    "                'max_score': ranking['similarity_score'].max()\n",
    "            }\n",
    "\n",
    "           # print top 5 candidates\n",
    "            print(f\"Top 5 candidates using {method.upper()}:\")\n",
    "            for _, row in top10.head(5).iterrows():\n",
    "                starred_mark = \" ⭐\" if row['id'] in (starred_candidates or []) else \"\"\n",
    "                print(f\"  {row['rank']:2d}. ID {row['id']:3d} - {row['job_title'][:50]:<50} (Score: {row['similarity_score']:.4f}){starred_mark}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "# Now let's get the best method\n",
    "def get_best_method(data, target_string, starred_candidates=None):\n",
    "    results=compare_methods(data, target_string, starred_candidates=starred_candidates)\n",
    "\n",
    "    #Find method with highest average score\n",
    "    best_method = max(results.keys(), key=lambda x:results[x]['avg_score'])\n",
    "\n",
    "    print(f\"Best Method : {best_method}\")\n",
    "    print(f\"Average similarity score : {results[best_method]['avg_score']:.4f}\")\n",
    "\n",
    "    return best_method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada3adb-e99a-4e90-a8a4-669c3dc2f191",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8056b29c-3dad-493a-83aa-d8393bbb8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Talent_ranker(data, target_string, starred_candidates=None):\n",
    "    # data preprocessing\n",
    "    data['job_title_preprocessed']=data['job_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "    # Finding the best method\n",
    "    best_method=get_best_method(data, target_string, starred_candidates=starred_candidates)\n",
    "\n",
    "    return best_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84dd259-94d1-473a-a962-e15c7d5ad738",
   "metadata": {},
   "source": [
    "Now let's test our function, first without starred candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "811c1bdf-751e-40ee-a84b-9cf010c66d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates using BOW:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.0000)\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.0000)\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.0000)\n",
      "   4. ID  73 - Aspiring Human Resources Manager, seeking internsh (Score: 0.8083)\n",
      "   5. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.7977)\n",
      "Top 5 candidates using TFIDF:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.0000)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 0.9913)\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 0.9913)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.7246)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.7246)\n",
      "Top 5 candidates using BERT:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.0000)\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.0000)\n",
      "   3. ID  99 - Seeking Human Resources Position                   (Score: 0.9760)\n",
      "   4. ID   4 - People Development Coordinator at Ryan             (Score: 0.9098)\n",
      "   4. ID  22 - People Development Coordinator at Ryan             (Score: 0.9098)\n",
      "Top 5 candidates using SBERT:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.0000)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 0.9938)\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 0.9938)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.8792)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.8792)\n",
      "Best Method : bert\n",
      "Average similarity score : 0.6621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bert'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_string = \"seeking human resources\"\n",
    "Talent_ranker(data, target_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9b4e1-79ee-4916-a537-9e60c9682912",
   "metadata": {},
   "source": [
    "It seems that BERT showed the highest average similarity score, but I think other method also performed pretty well judging by the results. Now let's try to add starred candidates to the mix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b089f3c-a68d-4809-833a-29ef48f1bb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates using BOW:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2398) ⭐\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2398)\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.2398)\n",
      "   4. ID  73 - Aspiring Human Resources Manager, seeking internsh (Score: 1.0486)\n",
      "   5. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.0367) ⭐\n",
      "Top 5 candidates using TFIDF:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.1949)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 1.1871) ⭐\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 1.1871)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.9257)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.9257) ⭐\n",
      "Top 5 candidates using BERT:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2522) ⭐\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2522)\n",
      "   3. ID  99 - Seeking Human Resources Position                   (Score: 1.2292)\n",
      "   4. ID   4 - People Development Coordinator at Ryan             (Score: 1.1639)\n",
      "   4. ID  22 - People Development Coordinator at Ryan             (Score: 1.1639)\n",
      "Top 5 candidates using SBERT:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.2504)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2445) ⭐\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2445)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.1312)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.1312) ⭐\n",
      "Best Method : bert\n",
      "Average similarity score : 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bert'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starred_candidates=[53, 28, 68]\n",
    "Talent_ranker(data, target_string, starred_candidates=starred_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01faed7d-2058-464f-b845-c6dc2546d51e",
   "metadata": {},
   "source": [
    "And again, BERT proved to be the best model. As a final step for the project, we'll put everything together in a separate class with all the functions, so that our client can use the model without running all these cells and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "665d5f38-a16e-4ff7-959b-56ce26a24a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveTalentRanker:\n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        \n",
    "        # Initialize NLP components\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        print(f\"Loaded {len(self.df)} candidates\")\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.df['job_title_processed'] = self.df['job_title'].apply(self.preprocess_text)\n",
    "        print(\"Data preprocessing completed\")\n",
    "\n",
    "    def bag_of_words_similarity(self, target_string):\n",
    " \n",
    "        print(\"Using Bag of Words...\")\n",
    "        \n",
    "        # Create BoW vectorizer\n",
    "        vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        \n",
    "        # Combine job titles with target string\n",
    "        all_texts = list(self.df['job_title_processed']) + [target_string]\n",
    "        bow_matrix = vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Calculate similarity between each job title and target\n",
    "        job_titles_matrix = bow_matrix[:-1]  # All except last (target)\n",
    "        target_vector = bow_matrix[-1:]      # Last row (target)\n",
    "        \n",
    "        similarities = cosine_similarity(job_titles_matrix, target_vector).flatten()\n",
    "        return similarities\n",
    "    \n",
    "    def tfidf_similarity(self, target_string):\n",
    "\n",
    "        print(\"Using TF-IDF...\")\n",
    "        \n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        \n",
    "        # Combine job titles with target string\n",
    "        all_texts = list(self.df['job_title_processed']) + [target_string]\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Calculate similarity between each job title and target\n",
    "        job_titles_matrix = tfidf_matrix[:-1]  # All except last (target)\n",
    "        target_vector = tfidf_matrix[-1:]      # Last row (target)\n",
    "        \n",
    "        similarities = cosine_similarity(job_titles_matrix, target_vector).flatten()\n",
    "        return similarities\n",
    "    \n",
    "    def bert_similarity(self, target_string):\n",
    "\n",
    "        print(\"Using BERT...\")\n",
    "        \n",
    "        # Load BERT model and tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model.eval()\n",
    "        \n",
    "        # Calculate BERT embeddings for job titles\n",
    "        doc_embeddings = []\n",
    "        for text in self.df['job_title']:\n",
    "            if pd.isna(text):\n",
    "                doc_embeddings.append(np.zeros(768))\n",
    "                continue\n",
    "                \n",
    "            inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "                doc_embeddings.append(embedding)\n",
    "        \n",
    "        # Calculate BERT embedding for target\n",
    "        target_inputs = tokenizer(target_string, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_outputs = model(**target_inputs)\n",
    "            target_embedding = target_outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for doc_embedding in doc_embeddings:\n",
    "            similarity = np.dot(doc_embedding, target_embedding) / (np.linalg.norm(doc_embedding) * np.linalg.norm(target_embedding) + 1e-8)\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return np.array(similarities)\n",
    "    \n",
    "\n",
    "    \n",
    "    def sbert_similarity(self, target_string):\n",
    "\n",
    "        print(\"Using Sentence-BERT...\")\n",
    "        \n",
    "        sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Get embeddings for job titles\n",
    "        job_titles = self.df['job_title'].fillna('').tolist()\n",
    "        job_embeddings = sbert_model.encode(job_titles)\n",
    "        \n",
    "        # Get embedding for target\n",
    "        target_embedding = sbert_model.encode([target_string])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(job_embeddings, target_embedding).flatten()\n",
    "        \n",
    "        return similarities\n",
    "\n",
    "    def rank_candidates(self, target_string, method='bert', starred_candidates=None):\n",
    "        \n",
    "        # Preprocess target string\n",
    "        target_processed = self.preprocess_text(target_string)\n",
    "        \n",
    "        # Calculate similarities based on the method\n",
    "        if method == 'bow':\n",
    "            similarities = self.bag_of_words_similarity(target_processed)\n",
    "        elif method == 'tfidf':\n",
    "            similarities = self.tfidf_similarity(target_processed)\n",
    "        elif method == 'bert':\n",
    "            similarities = self.bert_similarity(target_string)  \n",
    "        elif method == 'sbert':\n",
    "            similarities = self.sbert_similarity(target_string)  \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Available methods: 'bow', 'tfidf', 'bert', 'sbert'\")\n",
    "        \n",
    "        # Normalize similarities to 0-1 range\n",
    "        scaler = MinMaxScaler()\n",
    "        similarities_norm = scaler.fit_transform(similarities.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create ranking dataframe\n",
    "        ranking_df = self.df.copy()\n",
    "        ranking_df['similarity_score'] = similarities_norm\n",
    "        ranking_df['rank'] = ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "        \n",
    "        # Apply re-ranking if starred candidates provided\n",
    "        if starred_candidates:\n",
    "            ranking_df = self.rerank_with_starred(ranking_df, starred_candidates)\n",
    "        \n",
    "        # Sort by rank\n",
    "        ranking_df = ranking_df.sort_values('rank').reset_index(drop=True)\n",
    "        \n",
    "        return ranking_df\n",
    "    \n",
    "    def rerank_with_starred(self, ranking_df, starred_candidates):\n",
    "\n",
    "        print(f\"Re-ranking with {len(starred_candidates)} starred candidates...\")\n",
    "        \n",
    "        # Get features of starred candidates\n",
    "        starred_mask = ranking_df['id'].isin(starred_candidates)\n",
    "        starred_features = ranking_df[starred_mask]['similarity_score']\n",
    "        \n",
    "        if len(starred_features) == 0:\n",
    "            return ranking_df\n",
    "        \n",
    "        # Calculate average similarity of starred candidates\n",
    "        starred_avg = starred_features.mean()\n",
    "        \n",
    "        # Boost scores for candidates similar to starred ones\n",
    "        for idx, row in ranking_df.iterrows():\n",
    "            candidate_score = row['similarity_score']\n",
    "            \n",
    "            # Calculate similarity to starred candidates' average\n",
    "            similarity_to_starred = 1 - abs(candidate_score - starred_avg)\n",
    "            \n",
    "            # Apply boost\n",
    "            boost_factor = 1 + 0.3 * similarity_to_starred\n",
    "            ranking_df.loc[idx, 'similarity_score'] = candidate_score * boost_factor\n",
    "        \n",
    "        # Re-rank\n",
    "        ranking_df['rank'] = ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "        \n",
    "        return ranking_df\n",
    "    \n",
    "    def compare_methods(self, target_string, starred_candidates=None):\n",
    "\n",
    "        methods = ['bow', 'tfidf', 'bert', 'sbert']\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"Comparing methods for target: '{target_string}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\nTesting {method.upper()}...\")\n",
    "            try:\n",
    "                ranking = self.rank_candidates(target_string, method, starred_candidates)\n",
    "                \n",
    "                # Store top 10 results\n",
    "                top_10 = ranking.head(10)[['rank', 'id', 'job_title', 'similarity_score']]\n",
    "                results[method] = {\n",
    "                    'ranking': ranking,\n",
    "                    'top_10': top_10,\n",
    "                    'avg_score': ranking['similarity_score'].mean(),\n",
    "                    'max_score': ranking['similarity_score'].max()\n",
    "                }\n",
    "                \n",
    "                print(f\"Top 5 candidates using {method.upper()}:\")\n",
    "                for _, row in top_10.head(5).iterrows():\n",
    "                    starred_mark = \" ⭐\" if row['id'] in (starred_candidates or []) else \"\"\n",
    "                    print(f\"  {row['rank']:2d}. ID {row['id']:3d} - {row['job_title'][:50]:<50} (Score: {row['similarity_score']:.4f}){starred_mark}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_best_method(self, target_string, starred_candidates=None):\n",
    " \n",
    "        results = self.compare_methods(target_string, starred_candidates)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No methods worked successfully\")\n",
    "            return None\n",
    "        \n",
    "        # Find method with highest average score\n",
    "        best_method = max(results.keys(), key=lambda x: results[x]['avg_score'])\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"BEST METHOD: {best_method.upper()}\")\n",
    "        print(f\"Average similarity score: {results[best_method]['avg_score']:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return best_method\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4706d5f9-8afc-4bce-b846-733a65af8142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 104 candidates\n",
      "Preprocessing data...\n",
      "Data preprocessing completed\n",
      "Comparing methods for target: 'seeking human resources'\n",
      "============================================================\n",
      "\n",
      "Testing BOW...\n",
      "Using Bag of Words...\n",
      "Re-ranking with 3 starred candidates...\n",
      "Top 5 candidates using BOW:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2398) ⭐\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2398)\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.2398)\n",
      "   4. ID  73 - Aspiring Human Resources Manager, seeking internsh (Score: 1.0486)\n",
      "   5. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.0367) ⭐\n",
      "\n",
      "Testing TFIDF...\n",
      "Using TF-IDF...\n",
      "Re-ranking with 3 starred candidates...\n",
      "Top 5 candidates using TFIDF:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.1949)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 1.1871) ⭐\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 1.1871)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.9257)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 0.9257) ⭐\n",
      "\n",
      "Testing BERT...\n",
      "Using BERT...\n",
      "Re-ranking with 3 starred candidates...\n",
      "Top 5 candidates using BERT:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2522) ⭐\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2522)\n",
      "   3. ID  99 - Seeking Human Resources Position                   (Score: 1.2292)\n",
      "   4. ID   4 - People Development Coordinator at Ryan             (Score: 1.1639)\n",
      "   4. ID  22 - People Development Coordinator at Ryan             (Score: 1.1639)\n",
      "\n",
      "Testing SBERT...\n",
      "Using Sentence-BERT...\n",
      "Re-ranking with 3 starred candidates...\n",
      "Top 5 candidates using SBERT:\n",
      "   1. ID  99 - Seeking Human Resources Position                   (Score: 1.2504)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              (Score: 1.2445) ⭐\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              (Score: 1.2445)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.1312)\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi (Score: 1.1312) ⭐\n",
      "\n",
      "============================================================\n",
      "BEST METHOD: BERT\n",
      "Average similarity score: 0.8286\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "ranker=ComprehensiveTalentRanker(\"data/potential_talents_data.csv\")\n",
    "target = \"seeking human resources\"\n",
    "starred_candidates=[53, 28, 68]\n",
    "best_method = ranker.get_best_method(target, starred_candidates=starred_candidates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
