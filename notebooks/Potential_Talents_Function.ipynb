{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42997ec-bc27-4b34-8f9b-3c73c86c72fe",
   "metadata": {},
   "source": [
    "## The Final function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ccd2c-1afe-4105-965a-aa6051b3c769",
   "metadata": {},
   "source": [
    "In this notebook we'll put all the functions together into one ComprehensiveTalentRanker class, which, given the path to the data file, starred candidate ids and target string, will print the top 5 matching candidates info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf5af0e-e618-4a34-b2c7-581563721423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sirak\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import torch\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fd9212-b9b7-4cb9-b8d9-ea52b4bd5ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1129e9d0-e24f-4e0f-b6b8-c42c970a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveTalentRanker:\n",
    "    def __init__(self, data_path, glove_path=None, fasttext_path=None):\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.glove_embeddings = self.load_glove_embeddings(glove_path) if glove_path else None\n",
    "        self.fasttext_embeddings = self.load_fasttext_vectors(fasttext_path) if fasttext_path else None\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        print(f\"Loaded {len(self.df)} candidates\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.df['combined_string'] = self.df.apply(\n",
    "            lambda row: f\"{row['job_title']} {row['location']}\" if pd.notna(row['job_title']) and pd.notna(row['location']) else \"\",\n",
    "            axis=1\n",
    "        )\n",
    "        self.df['combined_string_processed'] = self.df['combined_string'].apply(self.preprocess_text)\n",
    "        print(\"Data preprocessing completed\")\n",
    "\n",
    "    # --- Embedding Loaders ---\n",
    "    def load_glove_embeddings(self, file_path):\n",
    "        if not file_path:\n",
    "            return None\n",
    "        embeddings = {}\n",
    "        with open(file_path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                embeddings[word] = vector\n",
    "        print(\"Loaded GloVe embeddings.\")\n",
    "        return embeddings\n",
    "\n",
    "    def load_fasttext_vectors(self, file_path, max_words=200000):\n",
    "        if not file_path:\n",
    "            return None\n",
    "        embeddings = {}\n",
    "        with open(file_path, 'r', encoding='utf8', newline='\\n', errors='ignore') as f:\n",
    "            next(f)  # skip header\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_words:\n",
    "                    break\n",
    "                parts = line.rstrip().split(' ')\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                embeddings[word] = vector\n",
    "        print(\"Loaded FastText embeddings.\")\n",
    "        return embeddings\n",
    "\n",
    "    # --- Embedding Methods ---\n",
    "    def bag_of_words_similarity(self, data, target_string):\n",
    "        vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        all_texts = list(data['combined_string_processed']) + [target_string]\n",
    "        bow_matrix = vectorizer.fit_transform(all_texts)\n",
    "        job_titles_matrix = bow_matrix[:-1]\n",
    "        target_vector = bow_matrix[-1:]\n",
    "        similarities = cosine_similarity(job_titles_matrix, target_vector).flatten()\n",
    "        return similarities\n",
    "\n",
    "    def tfidf_similarity(self, data, target_string):\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        all_texts = list(data['combined_string_processed']) + [target_string]\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "        job_titles_matrix = tfidf_matrix[:-1]\n",
    "        target_vector = tfidf_matrix[-1:]\n",
    "        similarities = cosine_similarity(job_titles_matrix, target_vector).flatten()\n",
    "        return similarities\n",
    "\n",
    "    def glove_similarity(self, data, target_string):\n",
    "        def get_document_embedding(text):\n",
    "            if pd.isna(text) or text == \"\":\n",
    "                return np.zeros(100)\n",
    "            text_processed = self.preprocess_text(text)\n",
    "            words = text_processed.split()\n",
    "            word_embeddings = [self.glove_embeddings[word] for word in words if word in self.glove_embeddings]\n",
    "            if len(word_embeddings) == 0:\n",
    "                return np.zeros(100)\n",
    "            return np.mean(word_embeddings, axis=0)\n",
    "        job_embeddings = [get_document_embedding(text) for text in data['combined_string']]\n",
    "        target_embedding = get_document_embedding(target_string)\n",
    "        similarities = cosine_similarity(job_embeddings, [target_embedding]).flatten()\n",
    "        return similarities\n",
    "\n",
    "    def fasttext_similarity(self, data, target_string):\n",
    "        def get_document_embedding(text):\n",
    "            if pd.isna(text) or text == \"\":\n",
    "                return np.zeros(300)\n",
    "            text_processed = self.preprocess_text(text)\n",
    "            words = text_processed.split()\n",
    "            word_embeddings = [self.fasttext_embeddings[word] for word in words if word in self.fasttext_embeddings]\n",
    "            if len(word_embeddings) == 0:\n",
    "                return np.zeros(300)\n",
    "            return np.mean(word_embeddings, axis=0)\n",
    "        job_embeddings = [get_document_embedding(text) for text in data['combined_string']]\n",
    "        target_embedding = get_document_embedding(target_string)\n",
    "        similarities = cosine_similarity(job_embeddings, [target_embedding]).flatten()\n",
    "        return similarities\n",
    "\n",
    "    def bert_similarity(self, data, target_string):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model.eval()\n",
    "        doc_embeddings = []\n",
    "        for text in data['combined_string']:\n",
    "            if pd.isna(text):\n",
    "                doc_embeddings.append(np.zeros(768))\n",
    "                continue\n",
    "            inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "                doc_embeddings.append(embedding)\n",
    "        target_inputs = tokenizer(target_string, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            target_outputs = model(**target_inputs)\n",
    "            target_embedding = target_outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "        similarities = [np.dot(doc_emb, target_embedding) / (np.linalg.norm(doc_emb) * np.linalg.norm(target_embedding) + 1e-8) for doc_emb in doc_embeddings]\n",
    "        return np.array(similarities)\n",
    "\n",
    "    def sbert_similarity(self, data, target_string):\n",
    "        sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        job_titles = data['combined_string'].fillna('').tolist()\n",
    "        job_embeddings = sbert_model.encode(job_titles)\n",
    "        target_embedding = sbert_model.encode([target_string])\n",
    "        similarities = cosine_similarity(job_embeddings, target_embedding).flatten()\n",
    "        return similarities\n",
    "\n",
    "    # --- Reranking Methods ---\n",
    "    def rerank_boosting(self, ranking_df, starred_candidates):\n",
    "        starred_mask = ranking_df['id'].isin(starred_candidates)\n",
    "        starred_features = ranking_df[starred_mask]['similarity_score']\n",
    "        if len(starred_features) == 0:\n",
    "            return ranking_df\n",
    "        starred_avg = starred_features.mean()\n",
    "        for idx, row in ranking_df.iterrows():\n",
    "            candidate_score = row['similarity_score']\n",
    "            similarity_to_starred = 1 - abs(candidate_score - starred_avg)\n",
    "            boost_factor = 1 + 0.3 * similarity_to_starred\n",
    "            ranking_df.loc[idx, 'similarity_score'] = candidate_score * boost_factor\n",
    "        ranking_df['rank'] = ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "        return ranking_df\n",
    "\n",
    "    def rerank_combining(self, ranking_df, target_string, starred_candidates, method):\n",
    "        starred_mask = ranking_df['id'].isin(starred_candidates)\n",
    "        starred_info = \" \".join(ranking_df.loc[starred_mask, 'combined_string'].fillna(\"\").tolist())\n",
    "        \n",
    "        # Combine with target_string\n",
    "        combined_target = f\"{target_string} {starred_info}\"\n",
    "        \n",
    "        temp_df = ranking_df.copy()\n",
    "        temp_df['combined_string'] = temp_df['combined_string'].fillna(\"\")\n",
    "        temp_df['combined_string_processed'] = temp_df['combined_string'].apply(self.preprocess_text)\n",
    "        if method == 'bow':\n",
    "            similarities = self.bag_of_words_similarity(temp_df, self.preprocess_text(target_string))\n",
    "        elif method == 'tfidf':\n",
    "            similarities = self.tfidf_similarity(temp_df, self.preprocess_text(target_string))\n",
    "        elif method == 'glove':\n",
    "            similarities = self.glove_similarity(temp_df, target_string)\n",
    "        elif method == 'fasttext':\n",
    "            similarities = self.fasttext_similarity(temp_df, target_string)\n",
    "        elif method == 'bert':\n",
    "            similarities = self.bert_similarity(temp_df, target_string)\n",
    "        elif method == 'sbert':\n",
    "            similarities = self.sbert_similarity(temp_df, target_string)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}.\")\n",
    "        scaler = MinMaxScaler()\n",
    "        similarities_norm = scaler.fit_transform(similarities.reshape(-1, 1)).flatten()\n",
    "        temp_df['similarity_score'] = similarities_norm\n",
    "        temp_df['rank'] = temp_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "        display_cols = ['rank', 'id', 'job_title', 'location', 'similarity_score']\n",
    "        return temp_df[display_cols + [col for col in temp_df.columns if col not in display_cols]]\n",
    "\n",
    "    # --- Main Ranking Function ---\n",
    "    def rank_candidates(self, target_string, method='bert', starred_candidates=None, reranking_method='boosting'):\n",
    "        target_processed = self.preprocess_text(target_string)\n",
    "        data = self.df.copy()\n",
    "        if method == 'bow':\n",
    "            similarities = self.bag_of_words_similarity(data, target_processed)\n",
    "        elif method == 'tfidf':\n",
    "            similarities = self.tfidf_similarity(data, target_processed)\n",
    "        elif method == 'glove':\n",
    "            similarities = self.glove_similarity(data, target_string)\n",
    "        elif method == 'fasttext':\n",
    "            similarities = self.fasttext_similarity(data, target_string)\n",
    "        elif method == 'bert':\n",
    "            similarities = self.bert_similarity(data, target_string)\n",
    "        elif method == 'sbert':\n",
    "            similarities = self.sbert_similarity(data, target_string)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Available: 'bow', 'tfidf', 'glove', 'fasttext', 'bert', 'sbert'\")\n",
    "        scaler = MinMaxScaler()\n",
    "        similarities_norm = scaler.fit_transform(similarities.reshape(-1, 1)).flatten()\n",
    "        ranking_df = data.copy()\n",
    "        ranking_df['similarity_score'] = similarities_norm\n",
    "        ranking_df['rank'] = ranking_df['similarity_score'].rank(ascending=False, method='min').astype(int)\n",
    "        if starred_candidates:\n",
    "            if reranking_method == 'boosting':\n",
    "                ranking_df = self.rerank_boosting(ranking_df, starred_candidates)\n",
    "            elif reranking_method == 'combining':\n",
    "                ranking_df = self.rerank_combining(ranking_df, target_string, starred_candidates, method)\n",
    "        ranking_df = ranking_df.sort_values('rank').reset_index(drop=True)\n",
    "        return ranking_df\n",
    "\n",
    "    # --- Comparison and Best Method ---\n",
    "    def compare_methods(self, target_string, starred_candidates=None, reranking_method='boosting'):\n",
    "        methods = ['bow', 'tfidf', 'glove', 'fasttext', 'bert', 'sbert']\n",
    "        results = {}\n",
    "        print(f\"Comparing methods for target: '{target_string}'\")\n",
    "        print(\"=\"*60)\n",
    "        for method in methods:\n",
    "            print(f\"\\nTesting {method.upper()}...\")\n",
    "            try:\n",
    "                ranking = self.rank_candidates(target_string, method, starred_candidates, reranking_method)\n",
    "                top_10 = ranking.head(10)[['rank', 'id', 'job_title', 'location', 'similarity_score']]\n",
    "                results[method] = {\n",
    "                    'ranking': ranking,\n",
    "                    'top_10': top_10,\n",
    "                    'avg_score': ranking['similarity_score'].mean(),\n",
    "                    'max_score': ranking['similarity_score'].max()\n",
    "                }\n",
    "                print(f\"Top 5 candidates using {method.upper()}:\")\n",
    "                for _, row in top_10.head(5).iterrows():\n",
    "                    starred_mark = \" ⭐\" if row['id'] in (starred_candidates or []) else \"\"\n",
    "                    print(f\"  {row['rank']:2d}. ID {row['id']:3d} - {row['job_title'][:50]:<50} | {row['location'][:30]:<30} (Score: {row['similarity_score']:.4f}){starred_mark}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def get_best_method(self, target_string, starred_candidates=None, reranking_method='boosting'):\n",
    "        results = self.compare_methods(target_string, starred_candidates, reranking_method)\n",
    "        if not results:\n",
    "            print(\"No methods worked successfully\")\n",
    "            return None\n",
    "        best_method = max(results.keys(), key=lambda x: results[x]['avg_score'])\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BEST METHOD: {best_method.upper()}\")\n",
    "        print(f\"Average similarity score: {results[best_method]['avg_score']:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "        return best_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e3afc8-f98f-4588-8c17-13bb41aa4b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe embeddings.\n",
      "Loaded FastText embeddings.\n",
      "Loaded 104 candidates\n",
      "Preprocessing data...\n",
      "Data preprocessing completed\n",
      "Comparing methods for target: 'seeking human resources'\n",
      "============================================================\n",
      "\n",
      "Testing BOW...\n",
      "Top 5 candidates using BOW:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 1.0000)\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 1.0000)\n",
      "   3. ID  73 - Aspiring Human Resources Manager, seeking internsh | Houston, Texas Area            (Score: 0.8936)\n",
      "   4. ID  99 - Seeking Human Resources Position                   | Las Vegas, Nevada Area         (Score: 0.8563)\n",
      "   5. ID  53 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8044)\n",
      "\n",
      "Testing TFIDF...\n",
      "Top 5 candidates using TFIDF:\n",
      "   1. ID  28 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 1.0000)\n",
      "   1. ID  30 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 1.0000)\n",
      "   3. ID  10 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8660) ⭐\n",
      "   3. ID  40 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8660)\n",
      "   3. ID  53 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8660)\n",
      "\n",
      "Testing GLOVE...\n",
      "Top 5 candidates using GLOVE:\n",
      "   1. ID  73 - Aspiring Human Resources Manager, seeking internsh | Houston, Texas Area            (Score: 1.0000)\n",
      "   2. ID  10 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.9902) ⭐\n",
      "   2. ID  53 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.9902)\n",
      "   2. ID  62 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.9902)\n",
      "   2. ID  40 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.9902)\n",
      "\n",
      "Testing FASTTEXT...\n",
      "Top 5 candidates using FASTTEXT:\n",
      "   1. ID  78 - Human Resources Generalist at Schwan's             | Amerika Birleşik Devletleri    (Score: 1.0000)\n",
      "   2. ID  73 - Aspiring Human Resources Manager, seeking internsh | Houston, Texas Area            (Score: 0.8705)\n",
      "   3. ID  94 - Seeking Human  Resources Opportunities. Open to tr | Amerika Birleşik Devletleri    (Score: 0.8653)\n",
      "   4. ID  10 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8559) ⭐\n",
      "   4. ID  53 - Seeking Human Resources HRIS and Generalist Positi | Greater Philadelphia Area      (Score: 0.8559)\n",
      "\n",
      "Testing BERT...\n",
      "Top 5 candidates using BERT:\n",
      "   1. ID  74 - Human Resources Professional                       | Greater Boston Area            (Score: 1.0000)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 0.9622)\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 0.9622)\n",
      "   4. ID   8 - HR Senior Specialist                               | San Francisco Bay Area         (Score: 0.9259)\n",
      "   4. ID  51 - HR Senior Specialist                               | San Francisco Bay Area         (Score: 0.9259) ⭐\n",
      "\n",
      "Testing SBERT...\n",
      "Top 5 candidates using SBERT:\n",
      "   1. ID  99 - Seeking Human Resources Position                   | Las Vegas, Nevada Area         (Score: 1.0000)\n",
      "   2. ID  28 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 0.9375)\n",
      "   2. ID  30 - Seeking Human Resources Opportunities              | Chicago, Illinois              (Score: 0.9375)\n",
      "   4. ID   3 - Aspiring Human Resources Professional              | Raleigh-Durham, North Carolina (Score: 0.9358)\n",
      "   4. ID  21 - Aspiring Human Resources Professional              | Raleigh-Durham, North Carolina (Score: 0.9358)\n"
     ]
    }
   ],
   "source": [
    "#Usage example\n",
    "\n",
    "ranker = ComprehensiveTalentRanker(\n",
    "    data_path=\"../data/potential_talents_data.csv\",\n",
    "    glove_path=\"../glove_data/glove.6B.100d.txt\",\n",
    "    fasttext_path=\"../fasttext_data/cc.en.300.vec\"\n",
    ")\n",
    "target = \"seeking human resources\"\n",
    "starred_candidates = [51, 12, 10]\n",
    "ranker.compare_methods(target, starred_candidates, reranking_method='combining') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
